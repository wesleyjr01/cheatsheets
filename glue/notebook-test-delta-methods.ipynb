{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "# AWS Glue Studio Notebook\n##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "markdown",
			"source": "#### Optional: Run this cell to see available notebook commands (\"magics\").\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%help",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 1.0.2 \n",
					"output_type": "stream"
				},
				{
					"output_type": "display_data",
					"data": {
						"text/markdown": "\n# Available Magic Commands\n\n## Sessions Magic\n\n----\n    %help                             Return a list of descriptions and input types for all magic commands. \n    %profile            String        Specify a profile in your aws configuration to use as the credentials provider.\n    %region             String        Specify the AWS region in which to initialize a session. \n                                      Default from ~/.aws/config on Linux or macOS, \n                                      or C:\\Users\\ USERNAME \\.aws\\config\" on Windows.\n    %idle_timeout       Int           The number of minutes of inactivity after which a session will timeout. \n                                      Default: 2880 minutes (48 hours).\n    %session_id_prefix  String        Define a String that will precede all session IDs in the format \n                                      [session_id_prefix]-[session_id]. If a session ID is not provided,\n                                      a random UUID will be generated.\n    %status                           Returns the status of the current Glue session including its duration, \n                                      configuration and executing user / role.\n    %session_id                       Returns the session ID for the running session. \n    %list_sessions                    Lists all currently running sessions by ID.\n    %stop_session                     Stops the current session.\n    %glue_version       String        The version of Glue to be used by this session. \n                                      Currently, the only valid options are 2.0, 3.0 and 4.0. \n                                      Default: 2.0.\n----\n\n## Selecting Session Types\n\n----\n    %streaming          String        Sets the session type to Glue Streaming.\n    %etl                String        Sets the session type to Glue ETL.\n    %glue_ray           String        Sets the session type to Glue Ray.\n    %session_type       String        Specify a session_type to be used. Supported values: streaming, etl and glue_ray. \n----\n\n## Glue Config Magic \n*(common across all session types)*\n\n----\n\n    %%configure         Dictionary    A json-formatted dictionary consisting of all configuration parameters for \n                                      a session. Each parameter can be specified here or through individual magics.\n    %iam_role           String        Specify an IAM role ARN to execute your session with.\n                                      Default from ~/.aws/config on Linux or macOS, \n                                      or C:\\Users\\%USERNAME%\\.aws\\config` on Windows.\n    %number_of_workers  int           The number of workers of a defined worker_type that are allocated \n                                      when a session runs.\n                                      Default: 5.\n    %additional_python_modules  List  Comma separated list of additional Python modules to include in your cluster \n                                      (can be from Pypi or S3).\n    %%tags        Dictionary          Specify a json-formatted dictionary consisting of tags to use in the session.\n    \n    %%assume_role Dictionary, String  Specify a json-formatted dictionary or an IAM role ARN string to create a session \n                                      for cross account access.\n                                      E.g. {valid arn}\n                                      %%assume_role \n                                      'arn:aws:iam::XXXXXXXXXXXX:role/AWSGlueServiceRole' \n                                      E.g. {credentials}\n                                      %%assume_role\n                                      {\n                                            \"aws_access_key_id\" : \"XXXXXXXXXXXX\",\n                                            \"aws_secret_access_key\" : \"XXXXXXXXXXXX\",\n                                            \"aws_session_token\" : \"XXXXXXXXXXXX\"\n                                       }\n----\n\n                                      \n## Magic for Spark Sessions (ETL & Streaming)\n\n----\n    %worker_type        String        Set the type of instances the session will use as workers. \n    %connections        List          Specify a comma separated list of connections to use in the session.\n    %extra_py_files     List          Comma separated list of additional Python files From S3.\n    %extra_jars         List          Comma separated list of additional Jars to include in the cluster.\n    %spark_conf         String        Specify custom spark configurations for your session. \n                                      E.g. %spark_conf spark.serializer=org.apache.spark.serializer.KryoSerializer\n----\n                                      \n## Magic for Ray Session\n\n----\n    %min_workers        Int           The minimum number of workers that are allocated to a Ray session. \n                                      Default: 1.\n    %object_memory_head Int           The percentage of free memory on the instance head node after a warm start. \n                                      Minimum: 0. Maximum: 100.\n    %object_memory_worker Int         The percentage of free memory on the instance worker nodes after a warm start. \n                                      Minimum: 0. Maximum: 100.\n----\n\n## Action Magic\n\n----\n\n    %%sql               String        Run SQL code. All lines after the initial %%sql magic will be passed\n                                      as part of the SQL code.  \n----\n\n\n## Amazon Q Magic \n\n----\n\n    %%chat              String        Query Amazon Q. All lines after the initial %%chat magic will be passed\n                                      to Amazon Q as part of your prompt. (For PySpark Kernel only)\n----\n"
					},
					"metadata": {}
				}
			]
		},
		{
			"cell_type": "code",
			"source": "%%configure -f\n{\n    \"datalake-formats\": \"delta\"\n}",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 1.0.2 \nThe following configurations have been updated: {'datalake-formats': 'delta'}\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "####  Run this cell to set up and start your interactive session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%idle_timeout 60\n%glue_version 4.0\n%worker_type G.1X\n%number_of_workers 2\n\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom pyspark.sql.session import SparkSession\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\nimport delta\n\n  \nsc = (\n        SparkSession.builder.config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n        .config(\n            \"spark.sql.catalog.spark_catalog\",\n            \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n        )\n        # Config to allow Spark to read/write dates from parquet correctly after version Spark 3.0, details in SPARK-31404\n        .config(\"spark.sql.legacy.parquet.datetimeRebaseModeInRead\", \"CORRECTED\")\n        .config(\"spark.sql.legacy.parquet.datetimeRebaseModeInWrite\", \"CORRECTED\")\n        .config(\"spark.sql.legacy.parquet.int96RebaseModeInRead\", \"CORRECTED\")\n        .config(\"spark.sql.legacy.parquet.int96RebaseModeInWrite\", \"CORRECTED\")\n        .config(\"spark.sql.avro.datetimeRebaseModeInWrite\", \"CORRECTED\")\n        .config(\"spark.sql.avro.datetimeRebaseModeInRead\", \"CORRECTED\")\n        .config(\"spark.databricks.delta.fixSchema.GlueCatalog\", \"true\")\n        .getOrCreate()\n    )\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Current idle_timeout is None minutes.\nidle_timeout has been set to 60 minutes.\nSetting Glue version to: 4.0\nPrevious worker type: None\nSetting new worker type to: G.1X\nPrevious number of workers: None\nSetting new number of workers to: 2\nTrying to create a Glue session for the kernel.\nSession Type: glueetl\nWorker Type: G.1X\nNumber of Workers: 2\nSession ID: 210dac77-28b9-4401-8857-db5145d65d4f\nApplying the following default arguments:\n--glue_kernel_version 1.0.2\n--enable-glue-datacatalog true\n--datalake-formats delta\nWaiting for session 210dac77-28b9-4401-8857-db5145d65d4f to get into ready status...\nSession 210dac77-28b9-4401-8857-db5145d65d4f has been created.\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### Example: Create a DynamicFrame from a table in the AWS Glue Data Catalog and display its schema\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "# Read Delta Table as Dynamic Frame\nold_delta_table_path = \"s3://dl-staging-full-data-lake-test/postgres/staging_db_postgres_newschema/addr/\"\nnew_delta_table_path = \"s3://dl-staging-full-data-lake-test/postgres/staging_db_postgres_newschema/new_addr_from_notebook/\"\nnew_table_name = \"new_addr\"\ndatabase=\"staging_db_postgres_newschema\"\ndyf = glueContext.create_data_frame.from_catalog(\n    database=database,\n    table_name=\"addr\",\n    additional_options={\n        \"path\": old_delta_table_path\n    }\n)\n\ndyf.printSchema()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "root\n |-- Op: string (nullable = true)\n |-- cdc_timestamp: string (nullable = true)\n |-- id: integer (nullable = true)\n |-- field_a: string (nullable = true)\n |-- field_b: string (nullable = true)\n |-- partition_0: string (nullable = true)\n\n/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:127: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "dyf.show(5)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "+---+--------------------+---+-------+-------+-----------+\n| Op|       cdc_timestamp| id|field_a|field_b|partition_0|\n+---+--------------------+---+-------+-------+-----------+\n|  I|2023-07-04 16:08:...|  4|     DD|     DD| 2002-01-01|\n|  I|2023-07-04 16:08:...|  6|     FF|     FF| 2002-01-01|\n|  I|2023-07-04 16:08:...|  7|     GG|     GG| 2002-01-01|\n|  I|2023-07-04 16:08:...|  3|     CC|     CC| 2002-01-01|\n|  I|2023-07-04 16:08:...|  9|     II|     II| 2002-01-01|\n+---+--------------------+---+-------+-------+-----------+\nonly showing top 5 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Convert Dynamic frame to PySpark DF\ndf = dyf.toDF(*dyf.columns)\n\ndf.show(5)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "+---+--------------------+---+-------+-------+-----------+\n| Op|       cdc_timestamp| id|field_a|field_b|partition_0|\n+---+--------------------+---+-------+-------+-----------+\n|  I|2023-07-04 16:08:...|  4|     DD|     DD| 2002-01-01|\n|  I|2023-07-04 16:08:...|  6|     FF|     FF| 2002-01-01|\n|  I|2023-07-04 16:08:...|  7|     GG|     GG| 2002-01-01|\n|  I|2023-07-04 16:08:...|  3|     CC|     CC| 2002-01-01|\n|  I|2023-07-04 16:08:...|  9|     II|     II| 2002-01-01|\n+---+--------------------+---+-------+-------+-----------+\nonly showing top 5 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Write PySpark df as new delta table - BROKEN\nnew_table_name = \"new_addr\"\ndatabase=\"staging_db_postgres_newschema\"\nadditional_options = {\n    \"path\": new_delta_table_path,\n    # \"mergeSchema\": True\n}\ndf.coalesce(1).write \\\n    .format(\"delta\") \\\n    .options(**additional_options) \\\n    .mode(\"overwrite\") \\\n    .saveAsTable(f\"{database}.{new_table_name}\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 9,
			"outputs": [
				{
					"name": "stdout",
					"text": "Py4JError: An error occurred while calling o153.toString. Trace:\njava.lang.IllegalArgumentException: object is not an instance of declaring class\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Write Delta table with Glue API - Broken\nnew_table_name = \"new_addr\"\ndatabase=\"staging_db_postgres_newschema\"\nnew_delta_table_path = \"s3://dl-staging-full-data-lake-test/postgres/staging_db_postgres_newschema/new_addr_from_notebook/\"\nglueContext.write_dynamic_frame.from_catalog(\n    frame=dyf,\n    database = database, \n    table_name = new_table_name, \n    transformation_ctx = \"datasource0\", \n    additional_options={\n        \"path\": new_delta_table_path\n    })",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 10,
			"outputs": [
				{
					"name": "stdout",
					"text": "Py4JJavaError: An error occurred while calling o83.getCatalogSink.\n: com.amazonaws.services.glue.model.EntityNotFoundException: Entity Not Found (Service: AWSGlue; Status Code: 400; Error Code: EntityNotFoundException; Request ID: d8c0f43f-2425-4a06-a269-c00b98fc2be3; Proxy: null)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1879)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1418)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1387)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1157)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:814)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:781)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:755)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:715)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:697)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:561)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:541)\n\tat com.amazonaws.services.glue.AWSGlueClient.doInvoke(AWSGlueClient.java:15984)\n\tat com.amazonaws.services.glue.AWSGlueClient.invoke(AWSGlueClient.java:15951)\n\tat com.amazonaws.services.glue.AWSGlueClient.invoke(AWSGlueClient.java:15940)\n\tat com.amazonaws.services.glue.AWSGlueClient.executeGetTable(AWSGlueClient.java:9062)\n\tat com.amazonaws.services.glue.AWSGlueClient.getTable(AWSGlueClient.java:9031)\n\tat com.amazonaws.services.glue.util.DataCatalogWrapper.$anonfun$getTable$2(DataCatalogWrapper.scala:205)\n\tat com.amazonaws.services.glue.util.LakeformationRetryWrapper$.$anonfun$executeWithRetry$1(DataCatalogWrapper.scala:1027)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.amazonaws.services.glue.util.LakeformationRetryWrapper$.executeWithRetry(DataCatalogWrapper.scala:1027)\n\tat com.amazonaws.services.glue.util.DataCatalogWrapper.$anonfun$getTable$1(DataCatalogWrapper.scala:204)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.amazonaws.services.glue.util.DataCatalogWrapper.getTable(DataCatalogWrapper.scala:170)\n\tat com.amazonaws.services.glue.GlueContext.getCatalogSink(GlueContext.scala:623)\n\tat com.amazonaws.services.glue.GlueContext.getCatalogSink(GlueContext.scala:614)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "s3output = glueContext.getSink(\n  path=\"s3://bucket_name/folder_name\",\n  connection_type=\"s3\",\n  updateBehavior=\"UPDATE_IN_DATABASE\",\n  partitionKeys=[],\n  compression=\"snappy\",\n  enableUpdateCatalog=True,\n  transformation_ctx=\"s3output\",\n)\ns3output.setCatalogInfo(\n  catalogDatabase=\"demo\", catalogTableName=\"populations\"\n)\ns3output.setFormat(\"glueparquet\")\ns3output.writeFrame(DyF)",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": null,
			"outputs": []
		}
	]
}